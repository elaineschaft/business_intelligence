---
title: "Class 05 Scraping a Single Web Page"
author: "Elaine Schaft"
date: "2026-02-16"
output: html_document
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
    code_folding: show
    code_download: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1 Intro to robotstxt
From the class activity, let us look at the https://www.indeed.com/jobs/ example.
```{r}
robotstxt::paths_allowed(
  paths= "/jobs/",
  domain = "https://www.indeed.com"
)
```
Given that this returned TRUE, it means that we are allowed to scrape this webpage (directory from the website).


# 2 Scraping the BLS webpage

In this example, we will be scraping the [Top Growing Jobs BLS Web Page] (https://www.bls.gov/emp/tables/fastest-growing-occupations.htm)

```{r web_page_bls}
# step 0: am I allowed to scrape this webpage
# this returned TRUE, which means I am amllowed to scrape the webpage
robotstxt::paths_allowed(
  paths= "/emp/",
  domain= "https://www.bls.gov"
)

# step 1: get the backend HTML pahe into R
# for this example, the simple rvest::read_html() failed wiht a "cannot open the connection" error, so we will start by declaring ourselves

# bls_fail = rvest::read_html(https://www.bls.gov/emp/tables/fastest-growing-occupations.htm)

url <- "https://www.bls.gov/emp/tables/fastest-growing-occupations.htm"
resp <- httr2::request(url) |>
  httr2::req_user_agent("ISA401 Course Scraper (Miami University)") |>
  # httr2::req_retry(max_tries = 3) |>
  # httr2::req_timeout(60) |>
  httr2::req_perform()

# if the previous step works correctly
# page will be a list of 2 that contains:
# (a) head [metadata]; and (b) body [things that we see]
page <- resp |> httr2::resp_body_raw() |> rvest::read_html()

# step 2: from the page, extract the part of interest
table_1 = page |> rvest::html_elements("#BLStable_2025_7_18_12_5")

# step 3: we make this human readable
table_1_clean = table_1 |> rvest::html_table()
table_1_clean = table_1_clean[[1]]

# alternate approaches for pulling that table from "page"
t1_b = # the page is step 1
  page |>
  # step 2
  rvest::html_elements(".regular") |>
  # step 3
  rvest::html_table() |>
  magrittr::extract2(1) # the extract2 fn is equivalent to the [[]]
```



## bls html on canvas

```{r}
# step 0 is confirming that I can scrape the directory/webpage
# Returned true so I can srape this
robotstxt::paths_allowed(
  paths = "/emp/",
  domain = "https://www.bls.gov/"
)

# step 1
resp = httr2::request(
  "https://www.bls.gov/emp/tables/fastest-growing-occupations.htm"
  ) |> 
  httr2::req_user_agent("An ISA 401 Bot from Miami University") |> 
  httr2::req_perform()

bls_webpage = resp |> httr2::resp_body_raw() |> rvest::read_html()
bls_webpage
```

